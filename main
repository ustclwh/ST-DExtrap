from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module
from utils import *
from data_process import *
from model import *
from itertools import chain

def calculate_random_walk_matrix(adj_mx):
    """返回随机游走邻接矩阵，用于D_GCN"""
    adj_mx = sp.coo_matrix(adj_mx)
    d = np.array(adj_mx.sum(1), dtype=float)
    d_inv = np.power(d, -1).flatten()
    d_inv[np.isinf(d_inv)] = 0.
    d_mat_inv = sp.diags(d_inv)
    random_walk_mx = d_mat_inv.dot(adj_mx).tocoo()
    return random_walk_mx.toarray()


class GraphConvolution(Module):
    def __init__(self, in_features_v, out_features_v, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features_v = in_features_v
        self.out_features_v = out_features_v

        # 定义权重参数
        self.weight = Parameter(torch.FloatTensor(in_features_v, out_features_v))
        self.p = Parameter(torch.FloatTensor(1, in_features_v))  # 用于邻接矩阵更新的节点特征交互
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features_v))
        else:
            self.register_parameter('bias', None)

        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, H_v, adj_v):
        interaction_effect = H_v @ self.p.t()
        interaction_matrix = interaction_effect @ interaction_effect.t()
        updated_adj_v = adj_v + interaction_matrix
        updated_adj_v = torch.clamp(updated_adj_v, 0, 1)

        output = torch.mm(updated_adj_v, torch.mm(H_v, self.weight))
        if self.bias is not None:
            output += self.bias
        return output, updated_adj_v

class GCN(nn.Module):
    def __init__(self, nfeat_v, nhid, time_length, dropout):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat_v, nhid)
        self.gc2 = GraphConvolution(nhid, nhid)
        self.gc3 = GraphConvolution(nhid, time_length)
        self.dropout = dropout

    def forward(self, X, adj_v):
        X, adj1 = self.gc1(X, adj_v)
        X = F.relu(X)
        X = F.dropout(X, self.dropout, training=self.training)

        X, adj2 = self.gc2(X, adj1)
        X = F.softmax(X)
        X = F.dropout(X, self.dropout, training=self.training)

        X, adj3 = self.gc3(X, adj2)
        return X, adj3

def test_error(model1,model2,model3,model4,h, unknow_set, test_data, A_s,Missing0,test_mask):
    """
    :unknow_set: The unknow locations for spatial prediction
    :test_data: The true value test_data of shape (test_num_timesteps, num_nodes)  保留着消失站点的数据
    :A_s: The full adjacent matrix
    :Missing0: True: 0 in original datasets means missing data
    :return: NAE, MAPE and RMSE
    """
    unknow_set = set(unknow_set)
    time_dim=4*h
    space_dim=test_data.shape[1]
    test_omask = np.ones(test_data.shape)
    if Missing0 == True:
        test_omask[test_data == 0] = 0
    test_inputs = (test_data * test_omask).astype('float32')
    test_inputs_s = test_inputs/E_maxvalue

    missing_index = np.ones(np.shape(test_data))

    missing_index[:, list(unknow_set)] = 0
    missing_index_s = missing_index

    o = np.zeros([test_data.shape[0] // time_dim * time_dim, test_inputs_s.shape[1],18])

    for i in range(0, test_data.shape[0]//time_dim*time_dim, time_dim):
        inputs = test_inputs_s[i:i+time_dim, :]
        missing_inputs = missing_index_s[i:i+time_dim, :]
        mask_all2=test_mask[i:i+time_dim, :]
        T_inputs = inputs*missing_inputs*mask_all2
        T_inputs = np.expand_dims(T_inputs, axis = 0)
        T_inputs = torch.from_numpy(T_inputs.astype('float32'))
        A_q = torch.from_numpy((calculate_random_walk_matrix(A_s).T).astype('float32'))
        A_h = torch.from_numpy((calculate_random_walk_matrix(A_s.T).T).astype('float32'))
        mask_2 = np.expand_dims(missing_inputs, axis = 0)
        mask_2=torch.from_numpy(mask_2.astype('float32'))
        imputation1=model1(T_inputs)
        imputation1 = imputation1.permute(2, 0, 1, 3).reshape(space_dim, -1)
        imputation2,A_s2=model2(imputation1, torch.from_numpy(A_s.astype('float32')))
        imputation2=imputation2.reshape(4,h*input_dim,-1)
        imputation3=model3(imputation2,A_q,A_h).reshape(time_dim,-1,space_dim).permute(0,2,1)
        imputation3 = imputation3.data.numpy()
        o[i:i+time_dim, :,:] = imputation3[0, :, :]

    o = o*E_maxvalue
    truth = test_inputs_s[0:test_data.shape[0] // time_dim * time_dim]*E_maxvalue
    test_mask2 =  (missing_index_s[0:test_data.shape[0]//time_dim*time_dim]==0 )|( test_mask[0:test_data.shape[0]//time_dim*time_dim]==0)

    if Missing0 == True:
        test_mask2[truth == 0] = 0
        o[truth == 0] = 0

    MAE = np.sum(np.abs(o - truth))/np.sum( test_mask2)
    RMSE = np.sqrt(np.sum((o - truth)*(o - truth))/np.sum( test_mask2) )
    MAPE = np.sum(np.abs(o - truth)/(truth + 1e-5))/np.sum( test_mask2)
    R2 = 1 - np.sum((o - truth) * (o- truth)) / np.sum((truth - truth.mean()) * (truth - truth.mean()))

    return MAE, RMSE, MAPE, R2,o, truth

def load_data(dataset):
    if dataset == 'chengdu':
        folder_path = ''
        X = load_and_concatenate_data(folder_path)
        max_length = 18
        data = X

        padded_data = np.zeros((data.shape[0], data.shape[1], max_length))
        mask = np.zeros((data.shape[0], data.shape[1], max_length))

        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                feature = data[i, j]
                feature_length = len(feature)
                padded_data[i, j, :feature_length] = feature
                mask[i, j, :feature_length] = 1
        X = padded_data
        A = np.load(r"/adj.npy")
    else:
        raise NotImplementedError('Please specify datasets from: chengdu')

    split_line1 = int(0.7 * X.shape[0])
    training_set = X[:split_line1, :]
    test_set = X[split_line1:, :]

    rand = np.random.RandomState(0)
    unknow_set = rand.choice(list(range(0, X.shape[1])), 451, replace=False)
    unknow_set = set(unknow_set)
    full_set = set(range(0, X.shape[1]))
    know_set = full_set - unknow_set

    training_set_s = training_set[:, list(know_set)]
    training_set_s[training_set_s.shape[0]-1:, :] = 0
    train_mask = mask[:split_line1, :][:, list(know_set)]
    test_mask = mask[split_line1:, :]
    A_s = A[:, list(know_set)][list(know_set), :]
    return A, X, training_set, test_set, unknow_set, full_set, know_set, training_set_s, A_s, train_mask, test_mask, mask

if __name__ == "__main__":
    batch_size = 4
    dataset = "chengdu"
    h = 30
    E_maxvalue = 2
    order = 2
    input_dim = 18
    nhead = 2
    hid_dim = 32
    nlayers = 2
    in_features_v = h
    hidden_features_v = 100
    out_features_v = h

    hidden_dim = 200
    dropout_rate = 0.1


    A, X, training_set, test_set, unknow_set, full_set, know_set, training_set_s, A_s, train_mask, test_mask, zong_mask = load_data(
        dataset)

    # 模型初始化
    model1 = TimeTransformerEncoder(input_dim, nhead, hid_dim, nlayers)
    model2 = GCN(batch_size * in_features_v * input_dim, hidden_features_v, batch_size * in_features_v * input_dim, dropout_rate)
    model3 = ThreeLayerDGCN(h*input_dim, hidden_dim, order)
    model4 = MultiScaleFrequencyEnhancement(input_dim * h)
    # ---------------------- 1. 模型参数导入部分 ----------------------
    # 示例：加载预训练参数（若存在则加载，否则从头训练）
    try:
          model1.load_state_dict(torch.load(r'/model1_best.pth'))
          model2.load_state_dict(torch.load(r'/model2_best.pth'))
          model3.load_state_dict(torch.load(r'/model3_best.pth'))
          model4.load_state_dict(torch.load(r'/model4_best.pth'))
          print("Successfully loaded pretrained parameters for all models.")
    except FileNotFoundError:
        print("Pretrained model parameters not found, training from scratch.")
    MAE_t, RMSE_t, MAPE_t, R2_t, pred, truth = test_error(model1,model2,model3,model4,h, unknow_set, test_set, A, False, test_mask)
    print(f" MAE: {MAE_t}, RMSE: {RMSE_t}")

