class TimeTransformerEncoder(nn.Module):
    def __init__(self, input_dim, nhead, hid_dim, nlayers):
        super(TimeTransformerEncoder, self).__init__()
        self.position_encoder = PositionalEncoding(input_dim)
        encoder_layers = TransformerEncoderLayer(input_dim, nhead, hid_dim, batch_first=True)  # 使用 batch_first=True
        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
        self.input_dim = input_dim

    def forward(self, src):
        # src 的形状为 [batch_size, time_steps, num_nodes, input_dim]
        batch_size, time_steps, num_nodes, input_dim = src.shape

        # 将 num_nodes 合并到 batch_size 中
        src = src.view(batch_size * num_nodes, time_steps, input_dim)  # 合并 num_nodes 到 batch_size

        # 添加位置编码
        src = self.position_encoder(src)

        # Transformer 编码器
        output = self.transformer_encoder(src)  # [batch_size * num_nodes, time_steps, input_dim]

        # 恢复形状
        output = output.view(batch_size, num_nodes, time_steps, input_dim)  # 恢复为 [batch_size, num_nodes, time_steps, input_dim]

        # 调整维度为 [batch_size, time_steps, num_nodes, input_dim]
        output = output.permute(0, 2, 1, 3).contiguous()
        if torch.isnan(output).any():
            print("NaN detected in 1")
        return output
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))

        # 填充正弦和余弦位置编码，确保索引不超过 div_term 的大小
        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位置
        if d_model % 2 == 1:
            # 如果 d_model 是奇数，最后一列保留未被填充的余弦值
            pe[:, 1::2] = torch.cos(position * div_term[:len(div_term)])
        else:
            pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # 调整为 [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x

class D_GCN(nn.Module):
    def __init__(self, in_channels, out_channels, orders, activation='relu'):
        super(D_GCN, self).__init__()
        self.orders = orders
        self.activation = activation
        self.num_matrices = 2 * self.orders + 1
        self.Theta1 = nn.Parameter(torch.FloatTensor(in_channels * self.num_matrices, out_channels))
        self.bias = nn.Parameter(torch.FloatTensor(out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.Theta1.shape[1])
        self.Theta1.data.uniform_(-stdv, stdv)
        stdv1 = 1. / math.sqrt(self.bias.shape[0])
        self.bias.data.uniform_(-stdv1, stdv1)

    def _concat(self, x, x_):
        x_ = x_.unsqueeze(0)
        return torch.cat([x, x_], dim=0)

    def forward(self, X, A_q, A_h):
        batch_size = X.shape[0]
        num_node = X.shape[1]
        input_size = X.size(2)
        supports = [A_q, A_h]

        x0 = X.permute(1, 2, 0)
        x0 = torch.reshape(x0, [num_node, input_size * batch_size])
        x = torch.unsqueeze(x0, 0)

        for support in supports:
            x1 = torch.mm(support, x0)
            x = self._concat(x, x1)
            for k in range(2, self.orders + 1):
                x2 = 2 * torch.mm(support, x1) - x0
                x = self._concat(x, x2)
                x1, x0 = x2, x1

        x = torch.reshape(x, [self.num_matrices, num_node, input_size, batch_size])
        x = x.permute(3, 1, 2, 0)
        x = torch.reshape(x, [batch_size, num_node, input_size * self.num_matrices])
        x = torch.matmul(x, self.Theta1)
        x += self.bias

        if self.activation == 'relu':
            x = F.relu(x)
        elif self.activation == 'selu':
            x = F.selu(x)
        elif self.activation == 'softmax':
            x = F.softmax(x)
        return x


# 定义 ThreeLayerDGCN
class ThreeLayerDGCN(nn.Module):
    def __init__(self, time_steps, hidden_dim, orders):
        super(ThreeLayerDGCN, self).__init__()
        self.time_steps = time_steps
        self.hidden_dim = hidden_dim
        self.orders = orders
        self.gcn1 = D_GCN(in_channels=time_steps, out_channels=hidden_dim, orders=orders)
        self.gcn2 = D_GCN(in_channels=hidden_dim, out_channels=hidden_dim, orders=orders,activation='relu')
        self.gcn3 = D_GCN(in_channels=hidden_dim, out_channels=time_steps, orders=orders, activation='linear')

    def forward(self, x, A_q, A_h):
        x = x.permute(0, 2, 1)  # 
        x1 = self.gcn1(x, A_q, A_h)
        x2 = self.gcn2(x1, A_q, A_h) + x1
        x3 = self.gcn3(x2, A_q, A_h)
        output = x3.permute(0, 2, 1)  # 
        return output
def dct(x, norm=None):
    if x.dim() == 2:
        N = x.size(1)
        x = x.contiguous().view(-1, N)  # 
        v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)
        Vc = torch.fft.rfft(v, dim=1)
        V = Vc.real
        if norm == 'ortho':
            V[:, 0] /= np.sqrt(N) * 2
            V[:, 1:] /= np.sqrt(N / 2) * 2
        V = 2 * V.view(-1, V.size(1))
        return V
    elif x.dim() == 3:
        batch_size, stations, time = x.size()
        x = x.contiguous().view(batch_size * stations, time)  # 
        V = dct(x, norm=norm)
        V = V.view(batch_size, stations, -1)
        return V
    else:
        raise ValueError("Input tensor must be 2D or 3D.")

class MultiScaleAttention(nn.Module):
    def __init__(self, input_len):
        super(MultiScaleAttention, self).__init__()
        self.input_len = input_len
        self.scale_1 = input_len // 2
        self.scale_2 = input_len // 4
        self.weight_full = nn.Parameter(torch.ones(1), requires_grad=True)
        self.weight_scale_1 = nn.Parameter(torch.ones(1), requires_grad=True)
        self.weight_scale_2 = nn.Parameter(torch.ones(1), requires_grad=True)

    def forward(self, x):
        batch_size, time, stations = x.shape
        full_scale = x
        scale_1 = nn.functional.adaptive_avg_pool1d(x.permute(0, 2, 1), self.scale_1).permute(0, 2, 1)
        scale_2 = nn.functional.adaptive_avg_pool1d(x.permute(0, 2, 1), self.scale_2).permute(0, 2, 1)
        weighted_full = self.weight_full * full_scale
        weighted_scale_1 = self.weight_scale_1 * scale_1
        weighted_scale_2 = self.weight_scale_2 * scale_2
        weighted_scale_1 = nn.functional.interpolate(weighted_scale_1.permute(0, 2, 1), size=time, mode='linear').permute(0, 2, 1)
        weighted_scale_2 = nn.functional.interpolate(weighted_scale_2.permute(0, 2, 1), size=time, mode='linear').permute(0, 2, 1)
        # 
        output = weighted_full + weighted_scale_1 + weighted_scale_2

        # 返回结果
        return output
class DCTChannelBlock(nn.Module):
    def __init__(self, time_length):
        super(DCTChannelBlock, self).__init__()

        self.time_length = time_length

        self.fc = nn.Sequential(
            nn.Linear(time_length // 2 + 1, time_length, bias=False),
            nn.Dropout(p=0.1),
            nn.ReLU(inplace=True),
            nn.Linear(time_length, time_length, bias=False),
            nn.Sigmoid()
        )
        self.layer_norm_dct = nn.LayerNorm([time_length // 2 + 1], eps=1e-6)

    def forward(self, x):
        batch_size, time, stations = x.size()

        dct_results = dct(x.permute(0, 2, 1))
        dct_results = dct_results.permute(0, 2, 1).contiguous()

        dct_size = time // 2 + 1
        dct_results = dct_results[:, :dct_size, :]
        dct_results = dct_results.permute(0, 2, 1)

        normed_dct = self.layer_norm_dct(dct_results)

        normed_dct = normed_dct.reshape(batch_size * stations, -1)

        attention_weights = self.fc(normed_dct)
        attention_weights = attention_weights.view(batch_size, stations, time).permute(0, 2, 1)

        normed_attention = nn.LayerNorm([stations], eps=1e-6)(attention_weights)

        output = x * normed_attention
        return output

class MultiScaleFrequencyEnhancement(nn.Module):
    def __init__(self, time_length):
        super(MultiScaleFrequencyEnhancement, self).__init__()
        self.multi_scale_attention = MultiScaleAttention(input_len=time_length)
        # 
        self.dct_channel_block = DCTChannelBlock(time_length=time_length)

    def forward(self, x):
        x = self.multi_scale_attention(x)
        x = self.dct_channel_block(x)
        return x
